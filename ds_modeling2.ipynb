{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Network (Broken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients keep exploading and it wouldn't use the loss function I built for it correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pprint\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.read_csv(\"song_list5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = songs[[\n",
    "    \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \n",
    "    \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \n",
    "    \"duration_ms\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(scaler, open('scaler2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork: \n",
    "    \n",
    "    def __init__(self):\n",
    "        # Setup Arch\n",
    "        self.inputs = 12 # Input Layer\n",
    "\n",
    "        self.encode1 = 512\n",
    "        self.encode2 = 256 # 3 Dense Layers\n",
    "        self.encode3 = 128\n",
    "\n",
    "        self.latent = 2 # Latent Space Dense Layer\n",
    "\n",
    "        self.decode1 = 128\n",
    "        self.decode2 = 256 # 3 Dense Layers\n",
    "        self.decode3 = 512\n",
    "\n",
    "        self.outputs = 12 # Output Layer\n",
    "        \n",
    "        # Initialize Weights\n",
    "        self.weights1 = np.random.randn(self.inputs, self.encode1)\n",
    "        self.weights2 = np.random.randn(self.encode1, self.encode2)\n",
    "        self.weights3 = np.random.randn(self.encode2, self.encode3)\n",
    "        self.weights4 = np.random.randn(self.encode3, self.latent)\n",
    "        self.weights5 = np.random.randn(self.latent, self.decode1)\n",
    "        self.weights6 = np.random.randn(self.decode1, self.decode2)\n",
    "        self.weights7 = np.random.randn(self.decode2, self.decode3)\n",
    "        self.weights8 = np.random.randn(self.decode3, self.outputs)\n",
    "\n",
    "    def relu(self, X):\n",
    "        return np.maximum(0,X)\n",
    "    def reluPrime(self, X):\n",
    "        X[X<=0] = 0\n",
    "        X[X>0] = 1\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        \"\"\"\n",
    "        # ===================Encoder========================\n",
    "        # Inputs to Layer 1\n",
    "        self.hidden_sum1 = np.dot(X, self.weights1)\n",
    "        self.activated_hidden1 = self.relu(self.hidden_sum1)\n",
    "        # Layer 1 to Layer 2\n",
    "        self.hidden_sum2 = np.dot(self.activated_hidden1, self.weights2)\n",
    "        self.activated_hidden2 = self.relu(self.hidden_sum2)\n",
    "        # Layer 2 to Layer 3\n",
    "        self.hidden_sum3 = np.dot(self.activated_hidden2, self.weights3)\n",
    "        self.activated_hidden3 = self.relu(self.hidden_sum3)\n",
    "        # ================Latent Space======================\n",
    "        # Layer 3 to Latent\n",
    "        self.hidden_sum4 = np.dot(self.activated_hidden3, self.weights4)\n",
    "        # ===================Decoder========================\n",
    "        # Latent to Layer 1\n",
    "        self.hidden_sum5 = np.dot(self.hidden_sum4, self.weights5)\n",
    "        self.activated_hidden4 = self.relu(self.hidden_sum5)\n",
    "        # Layer 1 to Layer 2\n",
    "        self.hidden_sum6 = np.dot(self.activated_hidden4, self.weights6)\n",
    "        self.activated_hidden5 = self.relu(self.hidden_sum6)\n",
    "        # Layer 2 to Layer 3\n",
    "        self.hidden_sum7 = np.dot(self.activated_hidden5, self.weights7)\n",
    "        self.activated_hidden6 = self.relu(self.hidden_sum7)\n",
    "        # Layer 3 to Output\n",
    "        self.hidden_sum8 = np.dot(self.activated_hidden6, self.weights8)\n",
    "\n",
    "        return self.hidden_sum8\n",
    "\n",
    "    def backward(self, X,o):\n",
    "        \"\"\"\n",
    "        Back prop thru the network\n",
    "        \"\"\"\n",
    "        \n",
    "        self.o_error = X-o\n",
    "        self.o_delta = self.o_error * self.reluPrime(self.hidden_sum8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, X,y,o):\n",
    "        \"\"\"\n",
    "        Back prop thru the network\n",
    "        \"\"\"\n",
    "        \n",
    "        self.o_error = y - o # Y - Output to get error\n",
    "        \n",
    "        # Apply derivative of sigmoid to error\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(self.output_sum) # Error * Weights(with activation)\n",
    "        \n",
    "        # z2 error: how much were our output layer weights off\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T) # Get the error of the Next layer\n",
    "        \n",
    "        # z2 delta: how much were the weights off?\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.hidden_sum) # error * Weights(with activation)\n",
    "\n",
    "        self.weights1 += X.T.dot(self.z2_delta) #Adjust first set (input => hidden) weights\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) #adjust second set (hidden => output) weights\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X,y,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return np.maximum(0,X)\n",
    "def reluPrime(X):\n",
    "    x = X.copy()\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_weights1 = np.random.randn(12, 512)\n",
    "test_weights2 = np.random.randn(512, 256)\n",
    "test_weights3 = np.random.randn(256, 128)\n",
    "test_weights4 = np.random.randn(128, 2)\n",
    "test_weights5 = np.random.randn(2, 128)\n",
    "test_weights6 = np.random.randn(128, 256)\n",
    "test_weights7 = np.random.randn(256, 512)\n",
    "test_weights8 = np.random.randn(512, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_test = np.atleast_2d(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing the prediction function code to ensure it is working\n",
    "\n",
    "# ===================Encoder========================\n",
    "# Input to Layer 1\n",
    "test_hidden_sum1 = np.dot(x_test, test_weights1)\n",
    "test_activated_hidden1 = relu(test_hidden_sum1)\n",
    "# Layer 1 to Layer 2\n",
    "test_hidden_sum2 = np.dot(test_activated_hidden1, test_weights2)\n",
    "test_activated_hidden2 = relu(test_hidden_sum2)\n",
    "# Layer 2 to Layer 3\n",
    "test_hidden_sum3 = np.dot(test_activated_hidden2, test_weights3)\n",
    "test_activated_hidden3 = relu(test_hidden_sum3)\n",
    "# ================Latent Space======================\n",
    "# Layer 3 to Latent\n",
    "test_hidden_sum4 = np.dot(test_activated_hidden3, test_weights4)\n",
    "# ===================Decoder========================\n",
    "# Latent to Layer 1\n",
    "test_hidden_sum5 = np.dot(test_hidden_sum4, test_weights5)\n",
    "test_activated_hidden4 = relu(test_hidden_sum5)\n",
    "# Layer 1 to Layer 2\n",
    "test_hidden_sum6 = np.dot(test_activated_hidden4, test_weights6)\n",
    "test_activated_hidden5 = relu(test_hidden_sum6)\n",
    "# Layer 2 to Layer 3\n",
    "test_hidden_sum7 = np.dot(test_activated_hidden5, test_weights7)\n",
    "test_activated_hidden6 = relu(test_hidden_sum7)\n",
    "# Layer 3 to Output\n",
    "test_hidden_sum8 = np.dot(test_activated_hidden6, test_weights8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing the backprop function\n",
    "\n",
    "learn_rate = .00000001\n",
    "\n",
    "# Error of the Output Layer\n",
    "output_error = x_test - test_hidden_sum8\n",
    "delta_output = output_error * test_hidden_sum8\n",
    "# Error of the Decoder Layer 3\n",
    "decode_3_error = delta_output.dot(test_weights8.T)\n",
    "delta_decode_3 = decode_3_error * reluPrime(test_hidden_sum7)\n",
    "# Error of the Decoder Layer 2\n",
    "decode_2_error = delta_decode_3.dot(test_weights7.T)\n",
    "delta_decode_2 = decode_2_error * reluPrime(test_hidden_sum6)\n",
    "# Error of the Decoder Layer 1\n",
    "decode_1_error = delta_decode_2.dot(test_weights6.T)\n",
    "delta_decode_1 = decode_1_error * reluPrime(test_hidden_sum5)\n",
    "# Error of the Latent Layer\n",
    "latent_error = decode_1_error.dot(test_weights5.T)\n",
    "delta_latent = latent_error * test_hidden_sum4\n",
    "# Error of the Encoder Layer 3\n",
    "encode_3_error = latent_error.dot(test_weights4.T)\n",
    "delta_encode_3 = encode_3_error * reluPrime(test_hidden_sum3)\n",
    "# Error of the Encoder Layer 2\n",
    "encode_2_error = encode_3_error.dot(test_weights3.T)\n",
    "delta_encode_2 = encode_2_error * reluPrime(test_hidden_sum2)\n",
    "# Error of the Encoder Layer 1\n",
    "encode_1_error = encode_2_error.dot(test_weights2.T)\n",
    "delta_encode_1 = encode_1_error * reluPrime(test_hidden_sum1)\n",
    "\n",
    "test_weights1 += x_test.T.dot(delta_encode_1) * learn_rate\n",
    "test_weights2 += test_activated_hidden1.T.dot(delta_encode_2) * learn_rate\n",
    "test_weights3 += test_activated_hidden2.T.dot(delta_encode_3) * learn_rate\n",
    "test_weights4 += test_activated_hidden3.T.dot(delta_latent) * learn_rate\n",
    "test_weights5 += test_hidden_sum4.T.dot(delta_decode_1) * learn_rate\n",
    "test_weights6 += test_activated_hidden4.T.dot(delta_decode_2) * learn_rate\n",
    "test_weights7 += test_activated_hidden5.T.dot(delta_decode_3) * learn_rate\n",
    "test_weights8 += test_activated_hidden6.T.dot(delta_output) * learn_rate\n",
    "\n",
    "print(abs(output_error).mean())\n",
    "print(abs(output_error.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pprint\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.read_csv(\"song_list5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = songs[[\n",
    "    \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \n",
    "    \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \n",
    "    \"duration_ms\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(scaler, open('scaler2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([-0.47695143, -1.43616823,  1.03467011, -1.32564479,  0.70711739,\n       -0.34028656,  1.91669133, -0.14321669,  3.87500921, -1.70714343,\n       -0.17066299,  0.03404049])"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reparameterization method for lambda layer\n",
    "# check this link out for research\n",
    "# https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_sigma) * epsilon \n",
    "    #      z_mean + e^(.5 * z_log_sigma)     * ϵ\n",
    "    #                      φ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"Encoder\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nEncoder_Input (InputLayer)      (None, 12)           0                                            \n__________________________________________________________________________________________________\nEncoder_Dense1 (Dense)          (None, 512)          6656        Encoder_Input[0][0]              \n__________________________________________________________________________________________________\nEncoder_Dense2 (Dense)          (None, 256)          131328      Encoder_Dense1[0][0]             \n__________________________________________________________________________________________________\nEncoder_Dense3 (Dense)          (None, 128)          32896       Encoder_Dense2[0][0]             \n__________________________________________________________________________________________________\nz_mean (Dense)                  (None, 2)            258         Encoder_Dense3[0][0]             \n__________________________________________________________________________________________________\nz_log_var (Dense)               (None, 2)            258         Encoder_Dense3[0][0]             \n__________________________________________________________________________________________________\nz (Lambda)                      (None, 2)            0           z_mean[0][0]                     \n                                                                 z_log_var[0][0]                  \n==================================================================================================\nTotal params: 171,396\nTrainable params: 171,396\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "# build encoder model\n",
    "encoder_input = Input(shape=(12,), name='Encoder_Input')\n",
    "\n",
    "encoder_dense1 = Dense(512, activation='relu', name='Encoder_Dense1')(encoder_input)\n",
    "encoder_dense2 = Dense(256, activation='relu', name='Encoder_Dense2')(encoder_dense1)\n",
    "encoder_dense3 = Dense(128, activation='relu', name='Encoder_Dense3')(encoder_dense2)\n",
    "\n",
    "# we need 2 Latent Sized Dense Layers for reparameterization\n",
    "z_mean = Dense(2, name='z_mean')(encoder_dense3)\n",
    "z_log_var = Dense(2, name='z_log_var')(encoder_dense3)\n",
    "\n",
    "# Use Lambda layer to apply the sampling function (Reparameterization)\n",
    "z = Lambda(sampling, output_shape=(2,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(encoder_input, [z_mean, z_log_var, z], name='Encoder')\n",
    "encoder.summary()\n",
    "# plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"Decoder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nz_sampling (InputLayer)      (None, 2)                 0         \n_________________________________________________________________\nDecoder_Dense1 (Dense)       (None, 128)               384       \n_________________________________________________________________\nDecoder_Dense2 (Dense)       (None, 256)               33024     \n_________________________________________________________________\nDecoder_Dense3 (Dense)       (None, 512)               131584    \n_________________________________________________________________\nDecoder_Output (Dense)       (None, 12)                6156      \n=================================================================\nTotal params: 171,148\nTrainable params: 171,148\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# build decoder model\n",
    "latent_inputs = Input(shape=(2,), name='z_sampling')\n",
    "\n",
    "decoder_dense1 = Dense(128, activation='relu', name='Decoder_Dense1')(latent_inputs)\n",
    "decoder_dense2 = Dense(256, activation='relu', name='Decoder_Dense2')(decoder_dense1)\n",
    "decoder_dense3 = Dense(512, activation='relu', name='Decoder_Dense3')(decoder_dense2)\n",
    "\n",
    "decoder_output = Dense(12, name='Decoder_Output')(decoder_dense3)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, decoder_output, name='Decoder')\n",
    "decoder.summary()\n",
    "# plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(encoder_input)[2])\n",
    "vae = Model(encoder_input, outputs, name='VAE_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"VAE_Model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nEncoder_Input (InputLayer)   (None, 12)                0         \n_________________________________________________________________\nEncoder (Model)              [(None, 2), (None, 2), (N 171396    \n_________________________________________________________________\nDecoder (Model)              (None, 12)                171148    \n=================================================================\nTotal params: 342,544\nTrainable params: 342,544\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# Reconstuction Loss Function\n",
    "reconstruction_loss = mse(encoder_input, outputs)\n",
    "reconstruction_loss *= 12\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()\n",
    "# plot_model(vae,\n",
    "#             to_file='vae_mlp.png',\n",
    "#             show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/20\n49985/49985 [==============================] - 7s 136us/step - loss: 8.6708\nEpoch 2/20\n49985/49985 [==============================] - 7s 135us/step - loss: 8.6318\nEpoch 3/20\n49985/49985 [==============================] - 7s 130us/step - loss: 8.5694\nEpoch 4/20\n49985/49985 [==============================] - 7s 130us/step - loss: 8.5559\nEpoch 5/20\n49985/49985 [==============================] - 7s 131us/step - loss: 8.5217\nEpoch 6/20\n49985/49985 [==============================] - 6s 130us/step - loss: 8.4926\nEpoch 7/20\n49985/49985 [==============================] - 6s 130us/step - loss: 8.4854\nEpoch 8/20\n49985/49985 [==============================] - 7s 132us/step - loss: 8.4570\nEpoch 9/20\n49985/49985 [==============================] - 7s 132us/step - loss: 8.4636\nEpoch 10/20\n49985/49985 [==============================] - 7s 132us/step - loss: 8.4522\nEpoch 11/20\n49985/49985 [==============================] - 7s 131us/step - loss: 8.4476\nEpoch 12/20\n49985/49985 [==============================] - 7s 140us/step - loss: 8.4139\nEpoch 13/20\n49985/49985 [==============================] - 6s 122us/step - loss: 8.4131\nEpoch 14/20\n49985/49985 [==============================] - 5s 108us/step - loss: 8.3883\nEpoch 15/20\n49985/49985 [==============================] - 5s 105us/step - loss: 8.3754\nEpoch 16/20\n49985/49985 [==============================] - 5s 106us/step - loss: 8.3673\nEpoch 17/20\n49985/49985 [==============================] - 5s 106us/step - loss: 8.3771\nEpoch 18/20\n49985/49985 [==============================] - 5s 106us/step - loss: 8.3664\nEpoch 19/20\n49985/49985 [==============================] - 6s 111us/step - loss: 8.3567\nEpoch 20/20\n49985/49985 [==============================] - 5s 105us/step - loss: 8.3702\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x232aeaf0eb8>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "vae.fit(x_train,\n",
    "        epochs=20,\n",
    "        batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(encoder, open('VAE_Encoder.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Method (Can't be Pickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pprint\n",
    "from pickle import dump\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "# from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.read_csv(\"song_list5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = songs[[\n",
    "    \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \n",
    "    \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \n",
    "    \"duration_ms\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(scaler, open('scaler2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(x_train[0])\n",
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(12,))\n",
    "\n",
    "encoder_dense1 = Dense(512, activation='relu')(encoder_input)\n",
    "encoder_dense2 = Dense(256, activation='relu')(encoder_dense1)\n",
    "encoder_dense3 = Dense(128, activation='relu')(encoder_dense2)\n",
    "\n",
    "encoder_output = Dense(2)(encoder_dense3)\n",
    "encoder = Model(encoder_input, encoder_output, name='Encoder')\n",
    "# encoder.compile()\n",
    "encoder.summary()\n",
    "# plot_model(encoder, to_file='encoder2.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(encoder.predict(np.atleast_2d(x_train[0])))\n",
    "# print(encoder.predict(np.atleast_2d(x_train[0])).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(2,))\n",
    "\n",
    "decoder_dense1 = Dense(128, activation='relu')(decoder_input)\n",
    "decoder_dense2 = Dense(256, activation='relu')(decoder_dense1)\n",
    "decoder_dense3 = Dense(512, activation='relu')(decoder_dense2)\n",
    "\n",
    "decoder_output = Dense(12)(decoder_dense3)\n",
    "# decoder_output = Dense(13, activation='softplus')(decoder_dense5)\n",
    "\n",
    "decoder = Model(decoder_input, decoder_output, name='Decoder')\n",
    "decoder.summary()\n",
    "# plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = decoder(encoder(encoder_input))\n",
    "auto_encoder = Model(encoder_input, outputs, name='Auto Encoder')\n",
    "\n",
    "\n",
    "\n",
    "# auto_encoder = Model(encoder_input, decoder_output, name='Auto Encoder')\n",
    "\n",
    "# auto_encoder = keras.Model(encoder_input, decoder_output)\n",
    "\n",
    "auto_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder.compile(optimizer='nadam',\n",
    "                    loss='mean_absolute_error',\n",
    "                    metrics='mean_absolute_percentage_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(auto_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auto_encoder.fit(x_train, x_train,\n",
    "                epochs=5,\n",
    "                batch_size=32,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = auto_encoder.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint.pprint(preds[0])\n",
    "pprint.pprint(x_train[0])\n",
    "print(\"=======================================================================\")\n",
    "pprint.pprint(preds[1])\n",
    "pprint.pprint(x_train[1])\n",
    "print(\"=======================================================================\")\n",
    "pprint.pprint(preds[2])\n",
    "pprint.pprint(x_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = encoder.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint.pprint(preds2[0:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save('encoder2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def __getstate__(self):\n",
    "#     state = self.__dict__.copy()\n",
    "#     del state['_metrics_lock']\n",
    "#     return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def __setstate__(self, state):\n",
    "#     self.__dict__.update(state)\n",
    "#     self.lock = threading.Lock() # ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.__dict__.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __setstate__(encoder, __getstate__(encoder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(encoder, open('encoder2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "unit4env",
   "display_name": "unit4env (Python3)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}